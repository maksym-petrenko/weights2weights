{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath(os.path.join(\"\", \"..\")))\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from lora_w2w import LoRAw2w, LoRAModule\n",
    "from utils import load_models, load_controlnet_models, inference, save_model_w2w, save_model_for_diffusers, unflatten, tensor_to_pil\n",
    "from inversion_utils_experimental import invert\n",
    "from diffusers import ControlNetModel\n",
    "device = \"cuda:0\"\n",
    "\n",
    "controlnet_model = load_controlnet_models(device)\n",
    "\n",
    "unet, vae, text_encoder, tokenizer, noise_scheduler = load_models(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.load(\"../files/mean.pt\").bfloat16().to(device)\n",
    "std = torch.load(\"../files/std.pt\").bfloat16().to(device)\n",
    "v = torch.load(\"../files/V.pt\").bfloat16().to(device)\n",
    "weight_dimensions = torch.load(\"../files/weight_dimensions.pt\")\n",
    "proj = torch.zeros(1,10000).bfloat16().to(device)\n",
    "network = LoRAw2w( proj, mean, std, v[:, :10000], \n",
    "                    unet,\n",
    "                    rank=1,\n",
    "                    multiplier=1.0,\n",
    "                    alpha=27.0,\n",
    "                    train_method=\"xattn-strict\"\n",
    "                ).to(device, torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"sks person\"\n",
    "negative_prompt = \"low quality, blurry, unfinished\" \n",
    "batch_size = 1\n",
    "height = 512\n",
    "width = 512\n",
    "controlnet_conditioning_scale = 0.8\n",
    "guidance_scale = 7\n",
    "seed = 42\n",
    "ddim_steps = 50\n",
    "generator = torch.Generator(device=device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = invert(network=network, unet=unet, vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, \n",
    "                 prompt=prompt, noise_scheduler = noise_scheduler, epochs=50, \n",
    "                 image_path = \"../inversion/images/deepfake\", \n",
    "                 mask_path = \"../inversion/images/deepfake_masks\", device = device, \n",
    "                 lr=0.2,\n",
    "                 weight_decay=0)\n",
    "\n",
    "if False:\n",
    "    for seed in [1, 42, 360, 1111]:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        \n",
    "        image = inference(network, unet, vae, text_encoder, tokenizer, prompt, negative_prompt, guidance_scale, noise_scheduler, ddim_steps, seed, generator, device)\n",
    "    \n",
    "        image = image.detach().cpu().float().permute(0, 2, 3, 1).numpy()[0]\n",
    "        image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "        image.save(f\"img_{seed}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_image_and_add_black_background(input_image_instance: Image.Image, scale_factor: float = 0.7) -> Image.Image:\n",
    "    original_image = input_image_instance.convert(\"RGBA\")\n",
    "    original_width, original_height = original_image.size\n",
    "    new_width, new_height = int(original_width * scale_factor), int(original_height * scale_factor)\n",
    "    scaled_image = original_image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    black_background = Image.new(\"RGBA\", (original_width, original_height), (0, 0, 0, 255))\n",
    "    x_offset, y_offset = (original_width - new_width) // 2, (original_height - new_height) // 2\n",
    "    black_background.paste(scaled_image, (x_offset, y_offset), scaled_image)\n",
    "    return black_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_sequence = []\n",
    "for i in range(128):\n",
    "    file = f\"deepfake_{i}.png\"\n",
    "    \n",
    "    input_image_for_canny = Image.open(f\"references/{file}\").convert(\"RGB\")\n",
    "    input_image_for_canny = scale_image_and_add_black_background(input_image_for_canny)\n",
    "\n",
    "    image_np = np.array(input_image_for_canny)\n",
    "    low_threshold = 100\n",
    "    high_threshold = 200\n",
    "    canny_edges_np = cv2.Canny(image_np, low_threshold, high_threshold)\n",
    "    control_image_pil = Image.fromarray(canny_edges_np).convert(\"RGB\")\n",
    "    \n",
    "    \n",
    "    openpose_sequence.append(control_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from controlnet_aux import CannyDetector, MidasDetector\n",
    "from utils import generate_video_sequence\n",
    "\n",
    "prompt = \"(sks person:1.25); portrait of a middle-aged woman with a serious expression, pale skin, slightly sunken cheeks, sharp jawline, subtle makeup, wearing a dark sleeveless top, platinum blonde hair tied back, neutral black background, realistic lighting, slightly stylized, soft shadows, high detail, photorealistic\"\n",
    "negative_prompt = \"blurry, distorted face, extra limbs, deformed eyes, cartoon, painting, unrealistic skin texture, low quality, glitch, waxy\"\n",
    "\n",
    "canny_processor = CannyDetector()\n",
    "depth_processor = MidasDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
    "\n",
    "canny_sequence = [canny_processor(img) for img in openpose_sequence]\n",
    "depth_sequence = [depth_processor(img) for img in openpose_sequence]\n",
    "\n",
    "frames = generate_video_sequence(\n",
    "    network=network,\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    guidance_scale=guidance_scale,\n",
    "    ddim_steps=ddim_steps,\n",
    "    device=device,\n",
    "    openpose_images=openpose_sequence,\n",
    "    canny_images=canny_sequence,\n",
    "    depth_images=depth_sequence,\n",
    "    controlnet_openpose=controlnet_model[\"openpose\"],\n",
    "    controlnet_canny=controlnet_model[\"canny\"],\n",
    "    controlnet_depth=controlnet_model[\"depth\"],\n",
    "    openpose_scale=0.7,\n",
    "    canny_scale=0.1,\n",
    "    depth_scale=0.1,\n",
    "    skip_first_frame_controlnet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for file in os.listdir(\"references\"):\n",
    "        input_image_for_canny = Image.open(f\"references/{file}\").convert(\"RGB\")\n",
    "        input_image_for_canny = scale_image_and_add_black_background(input_image_for_canny)\n",
    "    \n",
    "        image_np = np.array(input_image_for_canny)\n",
    "        low_threshold = 100\n",
    "        high_threshold = 200\n",
    "        canny_edges_np = cv2.Canny(image_np, low_threshold, high_threshold)\n",
    "        control_image_pil = Image.fromarray(canny_edges_np).convert(\"RGB\")\n",
    "    \n",
    "        img = inference_with_controlnet(\n",
    "            network=network,\n",
    "            unet=unet,\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            controlnet_model=controlnet_model,\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            control_image_pil=control_image_pil,\n",
    "            guidance_scale=guidance_scale,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            noise_scheduler=noise_scheduler,\n",
    "            ddim_steps=ddim_steps,\n",
    "            seed=seed,\n",
    "            generator=generator,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        image = img.detach().cpu().float().permute(0, 2, 3, 1).numpy()[0]\n",
    "        image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "        image.save(f\"deepfake/{file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, output_path, duration=100, loop=0):\n",
    "    \"\"\"\n",
    "    Save a list of tensor frames as an animated GIF\n",
    "    \n",
    "    Args:\n",
    "        frames: List of tensor frames from the inference function\n",
    "        output_path: Path to save the GIF file (e.g., \"output.gif\")\n",
    "        duration: Duration of each frame in milliseconds (default: 100ms = 10 FPS)\n",
    "        loop: Number of loops (0 = infinite loop)\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    \n",
    "    # Convert all tensor frames to PIL Images\n",
    "    pil_frames = []\n",
    "    for frame in frames:\n",
    "        pil_frame = tensor_to_pil(frame)\n",
    "        pil_frames.append(pil_frame)\n",
    "    \n",
    "    # Save as animated GIF\n",
    "    if pil_frames:\n",
    "        pil_frames[0].save(\n",
    "            output_path,\n",
    "            save_all=True,\n",
    "            append_images=pil_frames[1:],\n",
    "            duration=duration,\n",
    "            loop=loop,\n",
    "            optimize=True\n",
    "        )\n",
    "        print(f\"GIF saved to: {output_path}\")\n",
    "        print(f\"Frames: {len(pil_frames)}, Duration: {duration}ms per frame\")\n",
    "    else:\n",
    "        print(\"No frames to save!\")\n",
    "\n",
    "def save_frames_as_images(frames, output_dir, prefix=\"frame\", format=\"PNG\"):\n",
    "    \"\"\"\n",
    "    Save individual frames as separate image files\n",
    "    \n",
    "    Args:\n",
    "        frames: List of tensor frames from the inference function\n",
    "        output_dir: Directory to save images\n",
    "        prefix: Prefix for image filenames\n",
    "        format: Image format (PNG, JPEG, etc.)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, frame in enumerate(frames):\n",
    "        pil_frame = frame\n",
    "        filename = f\"{prefix}_{i}.{format.lower()}\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        pil_frame.save(filepath, format=format)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "    \n",
    "    print(f\"Saved {len(frames)} frames to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames_as_images(frames, \"deepfake\", prefix=\"deepfake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
